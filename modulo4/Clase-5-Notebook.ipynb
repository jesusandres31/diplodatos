{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diplomatura en Ciencia de Datos - UNNE - 2024**\n",
    "### Módulo 4: Aprendizaje Automático\n",
    "### Clase 5: Introducción a las Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos una red sencilla de una sola capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model with 1 hidden layer\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)  # Hidden layer\n",
    "        self.fc2 = nn.Linear(64, 1)   # Output layer\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))  # ReLU for the hidden layer\n",
    "        x = self.sigmoid(self.fc2(x))  # Sigmoid for the output layer (binary classification)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for data_batch, target_batch in train_loader:\n",
    "        output = model(data_batch)\n",
    "        loss = criterion(output, target_batch)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Manual weight updates\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor)\n",
    "        avg_test_loss = test_loss.item()\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Test the final model\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test_tensor)\n",
    "    test_output = test_output.round()  # Rounding the output to 0 or 1 for binary classification\n",
    "    accuracy = (test_output == y_test_tensor).float().mean()\n",
    "    print(f\"Final Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos ahora una red con dos capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=15, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model with 2 hidden layers and different activation functions\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)  # Second hidden layer\n",
    "        self.fc3 = nn.Linear(32, 1)   # Output layer\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))  # ReLU for the first hidden layer\n",
    "        x = self.tanh(self.fc2(x))  # Tanh for the second hidden layer\n",
    "        x = self.sigmoid(self.fc3(x))  # Sigmoid for output layer (binary classification)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = CustomNN()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for data_batch, target_batch in train_loader:\n",
    "        output = model(data_batch)\n",
    "        loss = criterion(output, target_batch)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Manual weight updates\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor)\n",
    "        avg_test_loss = test_loss.item()\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Test the final model\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test_tensor)\n",
    "    test_output = test_output.round()  # Rounding the output to 0 or 1 for binary classification\n",
    "    accuracy = (test_output == y_test_tensor).float().mean()\n",
    "    print(f\"Final Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regularizacion L1 para un caso de regresion**\n",
    "\n",
    "## **California Housing dataset**  \n",
    "\n",
    "Data Set Characteristics:  \n",
    "\n",
    "**Number of Instances:** 20640\n",
    "\n",
    "**Number of Attributes:** 8 numeric, predictive attributes and the target\n",
    "\n",
    "**Attribute Information:**\n",
    "- MedInc median income in block group\n",
    "- HouseAge median house age in block group\n",
    "- AveRooms average number of rooms per household\n",
    "- AveBedrms average number of bedrooms per household\n",
    "- Population block group population\n",
    "- AveOccup average number of household members\n",
    "- Latitude block group latitude\n",
    "- Longitude block group longitude\n",
    "\n",
    "**Missing Attribute Values:** None\n",
    "\n",
    "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000). This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(california_housing.data, california_housing.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class HousingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HousingNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model and loss function\n",
    "model_no_reg = HousingNN()\n",
    "model_l1_reg = HousingNN()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "l1_lambda = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# To track losses for plotting\n",
    "train_losses_no_reg = []\n",
    "train_losses_l1_reg = []\n",
    "\n",
    "# Training loop for both models\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss_no_reg = 0\n",
    "    total_train_loss_l1_reg = 0\n",
    "\n",
    "    for data_batch, target_batch in train_loader:\n",
    "        # Forward pass for model without regularization\n",
    "        output_no_reg = model_no_reg(data_batch)\n",
    "        loss_no_reg = criterion(output_no_reg, target_batch)\n",
    "\n",
    "        # Zero the gradients for the model without regularization\n",
    "        model_no_reg.zero_grad()\n",
    "\n",
    "        # Backpropagation for model without regularization\n",
    "        loss_no_reg.backward()\n",
    "\n",
    "        # Update weights manually for model without regularization\n",
    "        with torch.no_grad():\n",
    "            for param in model_no_reg.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss_no_reg += loss_no_reg.item()\n",
    "\n",
    "        # Forward pass for model with L1 regularization\n",
    "        output_l1_reg = model_l1_reg(data_batch)\n",
    "        loss_l1_reg = criterion(output_l1_reg, target_batch)\n",
    "\n",
    "        # Add L1 regularization\n",
    "        l1_penalty = 0\n",
    "        for param in model_l1_reg.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        loss_l1_reg += l1_lambda * l1_penalty\n",
    "\n",
    "        # Zero the gradients for the model with L1 regularization\n",
    "        model_l1_reg.zero_grad()\n",
    "\n",
    "        # Backpropagation for model with L1 regularization\n",
    "        loss_l1_reg.backward()\n",
    "\n",
    "        # Update weights manually for model with L1 regularization\n",
    "        with torch.no_grad():\n",
    "            for param in model_l1_reg.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss_l1_reg += loss_l1_reg.item()\n",
    "\n",
    "    # Record losses for each epoch\n",
    "    train_losses_no_reg.append(total_train_loss_no_reg / len(train_loader))\n",
    "    train_losses_l1_reg.append(total_train_loss_l1_reg / len(train_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, No Reg Loss: {total_train_loss_no_reg:.4f}, L1 Reg Loss: {total_train_loss_l1_reg:.4f}\")\n",
    "\n",
    "# Plotting the training losses\n",
    "plt.plot(train_losses_no_reg, label='No Regularization')\n",
    "plt.plot(train_losses_l1_reg, label='L1 Regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss with and without L1 Regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Iris con regularización L2**\n",
    "\n",
    "## **The Iris Dataset**\n",
    "\n",
    "This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.85, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class IrisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model and loss function\n",
    "model = IrisNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_reg = sum(param.norm(2) for param in model.parameters())\n",
    "        loss += weight_decay * l2_reg\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Manual weight updates\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor)\n",
    "        test_loss += weight_decay * sum(param.norm(2) for param in model.parameters())\n",
    "        test_errors.append(test_loss.item())\n",
    "\n",
    "    train_errors.append(total_train_loss / len(train_loader))\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {total_train_loss:.4f}, Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Plotting the errors\n",
    "plt.plot(train_errors, label='Training Error')\n",
    "plt.plot(test_errors, label='Testing Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Calidad de Vino\n",
    "\n",
    "**Input variables (based on physicochemical tests):**\n",
    "\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "\n",
    "**Output variable (based on sensory data):** \n",
    "\n",
    "12. quality (score between 0 and 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine = pd.read_csv(url, delimiter=\";\")\n",
    "\n",
    "# Split into features and labels\n",
    "X = wine.drop(\"quality\", axis=1).values\n",
    "y = wine[\"quality\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class WineQualityNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WineQualityNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(11, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)  # Wine quality can have 10 classes (0-9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model and loss function\n",
    "model = WineQualityNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_reg = sum(param.norm(2) for param in model.parameters())\n",
    "        loss += weight_decay * l2_reg\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Manual weight updates\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor)\n",
    "        test_loss += weight_decay * sum(param.norm(2) for param in model.parameters())\n",
    "        test_errors.append(test_loss.item())\n",
    "\n",
    "    train_errors.append(total_train_loss / len(train_loader))\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {total_train_loss:.4f}, Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Plotting the errors\n",
    "plt.plot(train_errors, label='Training Error')\n",
    "plt.plot(test_errors, label='Testing Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
